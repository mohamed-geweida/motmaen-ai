# Motmaen | Ù…ÙÙ€Ø·Ù€Ù…Ø¦ÙÙ€Ù†
<p align="center">
  <img src="assets/imgs/banner.png" alt="Motmaen AI Banner" width="100%" />
</p>

## ğŸš€ Overview

Motmaen AI is a deep-learning powered **Egyptian food image classification model** trained on a large, curated dataset of 10+ famous Egyptian dishes.
This model is part of the wider **Motmaen** ecosystem â€” a smart health assistant designed to support diabetic and chronic-disease patients by guiding nutrition choices with food recognition, personalized feedback, and future API integrations.

This repository contains:

* The **full dataset** (train/valid/test splits)
* The **entire training pipeline** (EDA, preprocessing, augmentation, training notebooks)
* The **final Keras model** and **TensorFlow Lite model** for deployment
* Scripts for exporting, predicting, and future API usage
* [Our Presentation](https://www.canva.com/design/DAG2iOg2Aig/3zS1O5Iuy0ErtlQO9XhvDA/edit?utm_content=DAG2iOg2Aig&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)
* [Demo Video](https://drive.google.com/file/d/1wItbhkH81M8Mwk08QXbhkX_xCFkQmEi_/view?usp=sharing)

---

## ğŸ“ Repository Structure

```
motmaen-ai/
â”œâ”€â”€ Models/                # Final trained models
â”‚   â”œâ”€â”€ best_final.keras
â”‚   â”œâ”€â”€ final_model.tflite
â”‚   â”œâ”€â”€ fine_tuned_model.keras
â”‚   â”œâ”€â”€ labels.txt
â”‚   â””â”€â”€ model_tflit_script (old).py
â”‚
â”œâ”€â”€ code/                  # Training & analysis notebooks
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â”œâ”€â”€ motmaen-final-model.ipynb
â”‚   â””â”€â”€ script.py
â”‚
â”œâ”€â”€ food photos/           # The processed dataset (train/valid/test)
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ test/
â”‚   â””â”€â”€ valid/
â”‚
â”œâ”€â”€ GI_Table.xlsx          # Nutritional table per food class
â””â”€â”€ README.md
```

The repo also contains earlier raw datasets under `/droped images/` for transparency and reproducibility.

---

# ğŸ² Supported Food Classes

The model currently recognizes the following Egyptian dishes:

* **Fattah (ÙØªØ©)**
* **Fool Medames (ÙÙˆÙ„)**
* **Hawawshy (Ø­ÙˆØ§ÙˆØ´ÙŠ)**
* **Koshari (ÙƒØ´Ø±ÙŠ)**
* **Kunafa (ÙƒÙ†Ø§ÙØ©)**
* **Mahshy El Kosa (Ù…Ø­Ø´ÙŠ ÙƒÙˆØ³Ø©)**
* **Roz Bel Laban (Ø±Ø² Ø¨Ù„Ø¨Ù† / Rice Pudding)**
* **Taameya (Ø·Ø¹Ù…ÙŠØ© / Falafel)**
* **Umm Ali (Ø£Ù… Ø¹Ù„ÙŠ)**
* **Baked Sweet Potato (Ø¨Ø·Ø§Ø·Ø§)**

Dataset size after cleaning & augmentation exceeds **15,000 images**.

---

# ğŸ§  Model Architecture

The final model is based on:

* **MobileNetV2** (pretrained on ImageNet)
* **Custom dense layers** optimized for 10-class classification
* **Mixed-precision training** for performance
* **Data augmentation** pipeline (rotation, flip, brightness, zoom, hue shifts, etc.)

The exported `.tflite` file is optimized for:

* Mobile CPUs
* Real-time inference
* Low latency (<30ms on mid-range phones)

---

# ğŸ“Š Training Pipeline

The full pipeline is available in:

* `code/EDA.ipynb` â€“ dataset analysis, cleaning, visualization
* `code/motmaen-final-model.ipynb` â€“ model building, training, evaluation
* `script.py` â€“ utilities for exporting & predicting

Key steps:

1. **Dataset Cleaning & Deduplication**
2. **Train/Valid/Test Splits**
3. **Image Augmentation**
4. **Transfer Learning + Fine Tuning**
5. **Evaluation (Accuracy, Confusion Matrix, F1)**
6. **TFLite Quantization (Float16)**

Accuracy achieved:

> â­ **~92% Top-1 Accuracy** on the final validation set.

---

# ğŸ“¦ How to Use

### 1ï¸âƒ£ Install dependencies

```bash
pip install tensorflow matplotlib numpy pillow
```

### 2ï¸âƒ£ Load the TFLite model

```python
import tensorflow as tf
import numpy as np
from PIL import Image

interpreter = tf.lite.Interpreter(model_path="Models/final_model.tflite")
interpreter.allocate_tensors()

# Get input-output layers
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
```

### 3ï¸âƒ£ Make a prediction

```python
img = Image.open("test_image.jpg").resize((224,224))
img = np.array(img, dtype=np.float32) / 255.0
img = np.expand_dims(img, 0)

interpreter.set_tensor(input_details[0]['index'], img)
interpreter.invoke()

prediction = interpreter.get_tensor(output_details[0]['index'])
print("Predicted class:", np.argmax(prediction))
```

---

# ğŸ§© How This Fits in the Motmaen Ecosystem

Motmaen is a larger unified platform designed to support:

### ğŸŒ **Future API (Planned)**

* /predict (upload food image â†’ return prediction)
* /nutrition (link prediction â†’ nutritional table)
* /profile recommendations (personalized diabetic guidance)

### ğŸ“± **Mobile App (Planned)**

Food scanning â†’ Nutrition estimation â†’ Dietary advice â†’ Progress tracking.

This repo provides the **vision foundation**: a reliable, optimized, deployable food recognition model.

---

# ğŸ”® Future Work

* Expand dataset to **30+ Egyptian dishes**
* Add calorie estimation & portion size detection
* Full FastAPI backend
* Mobile inference benchmarks (Android/iOS)
* Add ONNX export
* Model pruning for ultra-low-power devices

---

# ğŸ™Œ Contributors

* [Mohamed Geweida](https://www.github/mohamed-geweida/)
* [Eman Elnaggar](https://github.com/Eman-elnagggar)
* [Sohaila Mohamed](https://github.com/sohailamohamed15)
* [Shams Goda](https://github.com/usernameee111)
* [Noureen Ibrahim](https://github.com/noureen-156)
---

# ğŸ“œ License

This project is released under the **MIT License**.

---

# â­ Want to Support the Project?

Give the repo a **star** â­ on GitHub â€” it really helps with visibility as Motmaen grows into a full health assistant!

Just tell me!
